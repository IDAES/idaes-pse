{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29d3e74",
   "metadata": {},
   "source": [
    "# Custom Sparsification and Pruning Routines Using IDAES Sparsification and Pruning Utils\n",
    "\n",
    "The IDAES toolkit provides utility functions which allow the user to sparsify and prune Keras sequential neural networks (NNs). Sparsification is the process of setting a desired percentage of weights in each layer to zero. Sparse NNs are desired because they lower the inference time, allowing for faster prediction times. Pruning is the process of removing inactive nodes - nodes which do not contribute to the output - from the network entirely. Similar to sparsification, pruning will lower the inference time of NNs and will also decrease the size of the NN.\n",
    "\n",
    "## Initial Dependencies\n",
    "To begin the necessary libraries are imported for a simple NN training workflow and the IDAES utilities are imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries used to train and deploy NNs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import prod\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "\n",
    "# Import IDAES utility functions\n",
    "from prune import prune_sequential, count_nodes\n",
    "from sparsify import count_N_zero_weights, sparsify_sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f10ec09",
   "metadata": {},
   "source": [
    "## Building a Neural Network\n",
    "As a simple example data will be generated to train a NN which takes three inputs and returns one output. For this example the following non-linear model will be used:\n",
    "$$y = x_{1} * (x_{2} + x_{3})$$\n",
    "\n",
    "### Data Generation\n",
    "A uniform data set of a million data points is generated for the input vector $x$ and the output $y$ is calculated. After, the dataframe is obtained and separated into input and output dataframes. Typically a train/test split would be conducted when training a real NN, but for this example it is not necesarry as the accuracy and degree of overfitting of the neural network is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    df = pd.DataFrame(np.random.uniform(0, 1, size=(10**6, 3)), columns=['x1', 'x2', 'x3'])\n",
    "    df['y'] =  df['x1'] * (df['x2'] + df['x3'])\n",
    "    return df\n",
    "\n",
    "# Get the data necessary for the NN and separate into inputs and outputs\n",
    "df = get_data()\n",
    "print(df.describe())\n",
    "\n",
    "inputs = df[['x1', 'x2', 'x3']]\n",
    "outputs = df['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2670e783",
   "metadata": {},
   "source": [
    "### Model Formulation\n",
    "Currently, the IDAES utility functions support Keras Sequential models. For this example a relu model will be used with three hidden layers of 50 nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95296bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model that will be used to predict the outputs. For this example three hidden layers of 50 are used.\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(3))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adamax(learning_rate=0.1), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c46934",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling\n",
    "\n",
    "Sparsification routines are typically run with learning rate schedules. The sparsification schedule should match the learning rate schedule to some degree. In this example the learning rate will decrease by 2% each epoch. Keras learning rate routines are created using callbacks as shown here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd502d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a learning rate schedule for the neural network this reduces the learning rate by 2% per epoch\n",
    "class LearningRateReducer(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        lr = self.model.optimizer.lr.read_value()\n",
    "        lr = lr*0.98\n",
    "        self.model.optimizer.lr.assign(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b6c58e",
   "metadata": {},
   "source": [
    "After model formulation is complete the model and learning rate schedule objects can be instantiated for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2707f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network and run initial training steps, if training has already occurred t0 should be set.\n",
    "model = get_model()\n",
    "lr_schedule = LearningRateReducer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3950db7",
   "metadata": {},
   "source": [
    "## Initial Training of the Neural Network and Sparsification Routine\n",
    "\n",
    "A sparsification routine is run in tandem with training such that the sparsification schedule matches the learning rate schedule to a certain degree. In this example the sparsification routine follows the procedure proposed by [Zhu and Gupta 2017](https://arxiv.org/pdf/1710.01878.pdf):\n",
    "$$s_{t} = s_{f} + (s_{i} - s_{f}) * (1 - \\frac{t-t_{0}}{n\\Delta t})^{3}$$\n",
    "\n",
    "Where the sparsities $s_{i}$ is the initial sparsity, $s{t}$ is the sparsity after a sparsification step, and $s{f}$ is the desired final sparsification. Variable $t$ is the current training step, $\\Delta t$ is the timesteps between sparsification steps, and $t_{0}$ represents the number of training steps conducted prior to running sparsification. Finally, $n$ is the number of sparsification steps desired. It should be noted that $t_{0}$, $\\Delta t$, and $n$ are hyperparameters and can be adjusted to maintain accuracy during post-sparsification re-training. Parameters $t_{0}$ and $\\Delta t$ are training parameters set prior to the initial training of a NN and $n$ can be set after the initial training of a NN.\n",
    "\n",
    "For this example the initial training of the NN will train using 20 epochs with $\\Delta t = 1000$ steps per epoch. The initial timestep $t_{0} = epochs * \\Delta t$. Using these parameters we can train a neural network. By default a Keras epoch with $\\Delta t = N_{x} / N_{batch}$ where $N_{x}$ is the number of data points and $N_{batch}$ is the batch size. This allows the NN to train over all data points in batches of size $N_{batch}$. For this routine we will specify the training steps per epoch by setting the steps_per_epoch argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53eb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial training information\n",
    "epochs = 20\n",
    "dt = 1000\n",
    "t0 = epochs*dt\n",
    "\n",
    "# Initial training of the model\n",
    "model.fit(x=inputs, y=outputs, epochs=epochs, steps_per_epoch=dt, callbacks=[lr_schedule],\n",
    "          verbose=1, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c8d6b2",
   "metadata": {},
   "source": [
    "To compare the sparse model to the non-sparse model the keras evaluate function can be called to get a score for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4699258",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sparse_mse = model.evaluate(x=inputs, y=outputs, batch_size=64)\n",
    "print(f\"Initial model MSE on training data: {non_sparse_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243884a5",
   "metadata": {},
   "source": [
    "Since initial training was conducted the timestep before sparsification occurs is $t = t_{0}$. For this example an arbitrary number of sparsification steps was chosen as 15 and a desired sparsity of 50% was chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sparsification parameters t0 = steps/dt from training\n",
    "t = t0\n",
    "n_steps = 15\n",
    "sf = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb68055",
   "metadata": {},
   "source": [
    "To illustrate the effect of sparsification, a helper function is defined to count the total number of weights in the model. The sparsification util provides a helper function which counts the number of zero weights and was imported. Additionally, this function can be used to set the initial sparsity of the model in case there are weights equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to get the total number of weights in the neural net\n",
    "def get_num_weights(model):\n",
    "\n",
    "    # Get weights and biases\n",
    "    w = model.get_weights()\n",
    "\n",
    "    # Filter biases\n",
    "    w = [w[2*i] for i in range(int(len(w)/2))]\n",
    "\n",
    "    return sum([prod(l_w.shape) for l_w in w])\n",
    "\n",
    "N_weights = get_num_weights(model)\n",
    "N_zero_weights = count_N_zero_weights(model.get_weights())\n",
    "\n",
    "print(f\"Total weights in model: {N_weights}\")\n",
    "print(f\"Total number of zero weights in model: {N_zero_weights}\")\n",
    "\n",
    "# Define initial sparsity\n",
    "si = N_zero_weights/N_weights\n",
    "print(f\"Initial total model (not layer by layer) sparsity {si*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8bbfb3",
   "metadata": {},
   "source": [
    "With the initial training completed the sparsification loop can be generated. The sparsification util function takes a model and sparsifies to a provided sparsity. After a sparsification step is completed the timestep is incremented by $\\Delta t$ and the model is retrained. After sparsification is completed the model is retrained to reduce the loss of accuracy from sparsification. This training uses the sparse NN as the initial state and a reduced learning rate. Keras does not allow specific weights to be untrainable - only entire layers can be frozen- so the model is not retrained on the last sparsification step. Ideally, individual weights would be frozen such that the contribution from sparsified weights is shifted to non-sparsified weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom sparsification loop\n",
    "for n in range(1, n_steps + 1):\n",
    "\n",
    "    # Sparsification schedule can be modified to be whatever is desired\n",
    "    st = sf + (si - sf) * (1 - (t - t0) / (n_steps * dt)) ** 3\n",
    "    \n",
    "    # Sparsify to desired sparsification value, by default the model is modified inplace\n",
    "    model = sparsify_sequential(model, st, inplace=True)\n",
    "\n",
    "    # Update timestep\n",
    "    t += dt\n",
    "\n",
    "    # Retrain if not last step\n",
    "    if n != n_steps:\n",
    "        model.fit(x=inputs, y=outputs, epochs=1, steps_per_epoch=dt, callbacks=[lr_schedule],\n",
    "                  verbose=1, validation_split=0.2, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161e4a5",
   "metadata": {},
   "source": [
    "Note: If a modifying the original model is not desired the inplace parameter can be set to False. This requires manually recompiling the model before model.fit is called in the sparsification loop. To access the current learning rate for recompilation tf.keras.backend.eval(model.optimizer.lr) can be used.\n",
    "\n",
    "After sparsification the model error can be checked using the Keras evaluate function to compare to the full model accuracy,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_mse = model.evaluate(x=inputs, y=outputs, batch_size=64)\n",
    "print(f\"Final model mse after sparsification: {sparse_mse}\")\n",
    "print(f\"Error change after sparsification {non_sparse_mse - sparse_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_w = get_num_weights(model)\n",
    "zero_weights = count_N_zero_weights(model.get_weights())\n",
    "print(f\"Total Weights: {total_w}\\nZero Weights:{zero_weights}\\nSparsification:{zero_weights/total_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79abb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = prune_sequential(model, verbose=1)\n",
    "new_total_w = get_num_weights(pruned_model)\n",
    "print(f\"Total Weights Pruned Model: {new_total_w}\")\n",
    "print(f\"Total Reduction in Weights: {total_w - new_total_w} ({(total_w - new_total_w)/total_w*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8485fde0",
   "metadata": {},
   "source": [
    "The total number of nodes removed can also be found using the prune.count_nodes helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove output layer\n",
    "N_full = count_nodes(model) - 1\n",
    "N_pruned = count_nodes(pruned_model) -1 \n",
    "\n",
    "print(f\"Initial Model Nodes: {N_full}\")\n",
    "print(f\"Pruned Model Nodes: {N_pruned}\")\n",
    "print(f\"Reduction: {N_full - N_pruned} ({(N_full - N_pruned)/N_full*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce13c89",
   "metadata": {},
   "source": [
    "Since the model cannot be pruned inplace if future training/evaluation of the model is required it must be recompiled by the user. Depending on the model, a learning rate reset could be helpful as the model architecture was changed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
